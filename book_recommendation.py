# -*- coding: utf-8 -*-
"""Submission 2: Book Recommendation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1AmaRaxWH2KWdiybmkJioUjzb1N1Rq8K8

- Nama Lengkap : Adisaputra Zidha Noorizki
- Username : hi_zidha
- Email : hi.zidha@gmail.com

## Dataset Import

Menggunakan dataset bernama **Book-Crossing: User Review Ratings** yang dipublikasikan oleh Ruchi Bhatia yang dapat diakses pada [link ini](https://www.kaggle.com/datasets/ruchi798/bookcrossing-dataset)
"""

!pip install -q kaggle

"""Upload file **kaggle.json** untuk mentautkan dengan akun Kaggle"""

from google.colab import files
files.upload()

"""Import dataset yang diinginkan"""

! mkdir ~/.kaggle
! cp kaggle.json ~/.kaggle/
! chmod 600 ~/.kaggle/kaggle.json
! kaggle datasets download 'ruchi798/bookcrossing-dataset'

"""Unzip file yang telah di import"""

!unzip -o '/content/bookcrossing-dataset.zip'

"""## Data Understanding"""

import numpy as np
import pandas as pd

df_book = pd.read_csv('/content/Book reviews/Book reviews/BX_Books.csv', encoding = "ISO-8859-1", sep=";")
df_user = pd.read_csv('/content/Book reviews/Book reviews/BX-Users.csv', encoding = "ISO-8859-1", sep=";")
df_rating = pd.read_csv('/content/Book reviews/Book reviews/BX-Book-Ratings.csv', encoding = "ISO-8859-1", sep=";")
df_prepros = pd.read_csv('/content/Books Data with Category Language and Summary/Preprocessed_data.csv')

df_book.head()

df_book.info()

df_rating.head()

df_rating.info()

df_user.head()

df_user.info()

df_prepros.head()

df_prepros.info()

"""Dikarenakan data pada file 'BX_Books.csv', 'BX-Users.csv', dan 'BX-Book-Ratings.csv' tidak memiliki category sedangkan pada studi kasus ini akan membuat Sistem Rekomendasi Buku berdasarkan Category. Maka studi kasus akan menggunakan data 'Preprocessed_data'

### Univariate Exploratory Data Analysis

Menampilkan data rangkuman statistik deskriptif dari dataset
"""

df_prepros.describe()

unique_data = df_prepros.nunique()
print("Jumlah nilai unik:")
print(unique_data)

print('Banyak kategori buku: ', len(df_prepros.Category.unique()))
print('Kategori buku: ', df_prepros.Category.unique())

# Melihat jumlah data setiap kategori
df_prepros.Category.value_counts()

# Melihat ukuran dari df_prepros
df_prepros.shape

"""Menampilkan 10 data dengan jumlah teratas dan terbawah menurut data kategori"""

import matplotlib.pyplot as plt

# Menghitung jumlah kemunculan setiap kategori
category_counts = df_prepros.Category.value_counts()

# Mengambil 10 teratas dan 10 terbawah
category_top = category_counts.head(10)
category_bottom = category_counts.tail(10)

# 10 teratas
plt.barh(category_top.index, category_top.values, color='skyblue')
plt.title('Top 10 Categories')
plt.xlabel('Count')
plt.ylabel('Category')
plt.show()

# 10 terbawah
plt.barh(category_bottom.index, category_bottom.values, color='salmon')
plt.title('Bottom 10 Categories')
plt.xlabel('Count')
plt.ylabel('Category')
plt.show()

"""## Data Preprocessing"""

# Cek ukuran dari dataset kembali
df_prepros.shape

"""Mengidentifikasi nilai-nilai non-string dan mengambil tindakan yang sesuai untuk membersihkannya."""

df_preprocessing = df_prepros

# Memeriksa nilai-nilai non-string
unique_values = df_preprocessing.Category.unique()
non_string_values = [value for value in unique_values if not isinstance(value, str)]
print("Non-string values:", non_string_values)

# Menampilkan jumlah nilai non-string
print("Number of non-string values:", len(non_string_values))

# Menghapus nilai non-string
df_preprocessing.Category = df_preprocessing.Category.str.replace("[", "").str.replace("]", "").str.replace("'", "")

# Menampilkan hasil pembersihan
df_preprocessing.Category.head()

"""Mengidentifikasi nilai yang bukan huruf pada fitur 'Category' dan melakukan tindakan yang sesuai."""

# Mencocokkan nilai yang bukan huruf dalam kolom 'Category' kecuali '&' dan ','
non_alphabetic_values = df_preprocessing[df_preprocessing['Category'].str.contains(r'[^a-zA-Z\s&,]')]
non_alphabetic_values.Category

df_preprocessing = df_preprocessing.drop(non_alphabetic_values.index)

# Memastikan data kembali
df_preprocessing[df_preprocessing.Category.str.contains(r'[^a-zA-Z\s&,]')].Category

# Menghapus non-huruf pada fitur book_title
non_alphabetic_values_2 = df_preprocessing[df_preprocessing['book_title'].str.contains(r'[^a-zA-Z\s&,]')]
df_preprocessing = df_preprocessing.drop(non_alphabetic_values_2.index)

# Cek apakah ada data NaN atau tidak
df_preprocessing.isna().sum()

# Cek kembali ukuran data setelah pre-processing
df_preprocessing.shape

"""## Data Preparation

### Overview

Dikarenakan terlalu banyak baris dan data yang berbeda pada fitur 'Category', maka pada studi kasus kali ini akan menggunakan 100 data category dengan jumlah data teratas saja, serta hanya mengambil 15 judul dengan index teratas untuk setiap 'Category'-nya
"""

# Menghitung jumlah kemunculan setiap kategori
categoryC = df_preprocessing.Category.value_counts()

# Mengambil 100 kategori teratas
category_counts = categoryC.head(100).index.tolist()

# Menginisialisasi DataFrame kosong untuk menampung hasil
data_cleaned = pd.DataFrame()

# Memfilter data berdasarkan kategori teratas dan membatasi setiap kategori hingga maksimal 15 judul buku berbeda
for catts in category_counts:
    df_catt = df_preprocessing[df_preprocessing.Category == catts]
    title_unique = df_catt['book_title'].unique()[:15]
    data_cleaned = pd.concat([data_cleaned, df_catt[df_catt.book_title.isin(title_unique)]])

# Menampilkan ukuran data yang telah dibersihkan
print(data_cleaned.shape)

data_cleaned.Category.value_counts()

# Melihat data dengan index terakhir pada dataframe
data_cleaned.tail()

# Inisiasi variabel data_books yang berisi dataframe dengan data yang akan digunakan saja
data_books = data_cleaned.drop(columns=['Unnamed: 0', 'location', 'age', 'img_s', 'img_m', 'img_l', 'Language', 'city', 'state', 'country'])
data_books

"""Merubah data agar menjadi header menjadi **lowercase** dan data bertipe object menjadi **propercase**"""

# Mengonversi header menjadi proper case
data_books.columns = data_books.columns.str.lower()

# Mengonversi data menjadi proper case
data_books = data_books.apply(lambda x: x.str.title() if x.dtype == "object" else x)
data_books.head()

# Cek apakah ada missing value
data_books.isna().sum()

"""### Data Preparation: Content Based Filtering"""

# Membuat variabel data_cbf yang berisi dataframe data_books
data_cbf = data_books.drop(columns=['user_id'])

# Reindexing kolom pada dataframe
data_cbf = data_cbf.reindex(columns=['isbn','book_title','summary','book_author','publisher','year_of_publication','rating','category'])
data_cbf.head()

"""Mencari rata-rata rating dari setiap judul buku dan menghilangkan data duplikat pada fitur 'book_title'"""

# Menghitung rata-rata rating untuk setiap judul buku melalui nilai isbn
avg_rating = data_cbf.groupby('book_title')['rating'].mean().round(1)

# Memetakan rata-rata rating ke DataFrame asli berdasarkan 'book_title'
data_cbf.rating = data_cbf.book_title.map(avg_rating)

print(data_cbf.shape)

# Menghapus data duplikat berdasarkan 'book_title' setelah mengambil rata-rata rating
data_cbf = data_cbf.drop_duplicates(subset='book_title')

print(data_cbf.shape)

# Membuang data duplikat pada variabel data_cbf
data_cbf = data_cbf.drop_duplicates('isbn')
data_cbf.head()

# Menampilka rating tertinggi dan terendah
# Nilai rating tertinggi
rating_max = data_cbf.rating.max()

# Nilai rating terendah
rating_min = data_cbf.rating.min()

# Menampilkan hasil
print("Nilai rating tertinggi:", rating_max)
print("Nilai rating terendah:", rating_min)

"""### Data Preparation: Collaborative Filtering"""

# Mengambil subset DataFrame dengan kolom yang diinginkan dan disimpan pada data_rating
data_rating = data_books[['user_id', 'isbn', 'rating']].copy()

# Mengambil subset DataFrame dengan kolom yang diinginkan dan disimpan pada data_books_new
# Menghapus duplikat berdasarkan 'isbn' dan 'book_title'
data_books_new = data_books[['isbn', 'book_title', 'category']].copy()
data_books_new.drop_duplicates(subset=['isbn', 'book_title'], inplace=True)

# Duplikasi dataframe ke dalam data_cll
data_cll = data_rating
data_cll

"""Mengubah user_id dan isbn menjadi list tanpa nilai yang sama, serta melakukan proses encoding"""

# Mengubah user_id dan isbn menjadi list tanpa nilai yang sama
user_ids = data_cll.user_id.unique().tolist()
isbn_ids = data_cll.isbn.unique().tolist()

# Melakukan encoding user_id dan encoding angka ke user_id
user_to_user_encoded = {x: i for i, x in enumerate(user_ids)}
user_encoded_to_user = {i: x for i, x in enumerate(user_ids)}

# Melakukan encoding isbn dan encoding angka ke ke isbn
isbn_to_isbn_encoded = {x: i for i, x in enumerate(isbn_ids)}
isbn_encoded_to_isbn = {i: x for i, x in enumerate(isbn_ids)}

print('list user_id: ', user_ids)
print('list isbn: ', isbn_ids)
print('encoded user_id : ', user_to_user_encoded)
print('encoded angka ke user_id: ', user_encoded_to_user)
print('encoded isbn : ', isbn_to_isbn_encoded)
print('encoded angka ke isbn: ', isbn_encoded_to_isbn)

## Petakan user_id dan isbn ke dataframe yang berkaitan.

# Mapping user_id ke dataframe user
data_cll['user'] = data_cll['user_id'].map(user_to_user_encoded)

# Mapping isbn ke dataframe isbn_n
data_cll['isbn_n'] = data_cll['isbn'].map(isbn_to_isbn_encoded)
data_cll

## cek beberapa hal dalam data

# Mendapatkan jumlah user
num_users = len(user_to_user_encoded)

# Mendapatkan jumlah isbn
num_isbn = len(isbn_encoded_to_isbn)

# Mengubah rating menjadi nilai float
data_cll['rating'] = data_cll['rating'].values.astype(np.float32)

# Nilai minimum rating
min_rating = min(data_cll['rating'])

# Nilai maksimal rating
max_rating = max(data_cll['rating'])

print('Number of User: {}, Number of Resto: {}, Min Rating: {}, Max Rating: {}'
  .format(
    num_users, num_isbn, min_rating, max_rating
))

# Mengacak dataset
data_cll = data_cll.sample(frac=1, random_state=42)
data_cll

# Membuat variabel x untuk mencocokkan data user dan isbn menjadi satu value
x = data_cll[['user', 'isbn_n']].values

# Membuat variabel y untuk membuat rating dari hasil
y = data_cll['rating'].apply(lambda x: (x - min_rating) / (max_rating - min_rating)).values

# Split dataset menjadi 80% data train dan 20% data validasi
train_indices = int(0.8 * data_cll.shape[0])
x_train, x_val, y_train, y_val = (
    x[:train_indices],
    x[train_indices:],
    y[:train_indices],
    y[train_indices:]
)
print(x, y)

"""## Modelling

### Modelling: Content Based Filtering
"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Inisialisasi TfidfVectorizer
tf = TfidfVectorizer()

# Melakukan perhitungan idf pada data category
tf.fit(data_cbf['category'])

# Mapping array dari fitur index integer ke fitur nama
tf.get_feature_names_out()

# Melakukan fit lalu ditransformasikan ke bentuk matrix
tfidf_matrix = tf.fit_transform(data_cbf.category)

# Melihat ukuran matrix tfidf
tfidf_matrix.shape

# Mengubah vektor tf-idf dalam bentuk matriks dengan fungsi todense()
tfidf_matrix.todense()

# Membuat dataframe untuk melihat tf-idf matrix
# Kolom diisi dengan jenis category
# Baris diisi dengan judul buku

pd.DataFrame(
    tfidf_matrix.todense(),
    columns=tf.get_feature_names_out(),
    index=data_cbf.book_title
).sample(22, axis=1).sample(10, axis=0)

# mengidentifikasi korelasi antara judul buku dengan kategorinya.
from sklearn.metrics.pairwise import cosine_similarity

# Menghitung cosine similarity pada matrix tf-idf
cosine_sim = cosine_similarity(tfidf_matrix)
cosine_sim

# Membuat dataframe dari variabel cosine_sim dengan baris dan kolom berupa judul buku
cosine_sim_df = pd.DataFrame(cosine_sim, index=data_cbf.book_title, columns=data_cbf.book_title)
print('Shape:', cosine_sim_df.shape)

# Melihat similarity matrix pada setiap buku
cosine_sim_df.sample(5, axis=1).sample(10, axis=0)

def book_recommendations(book_title,
                         k,
                         similarity_data=cosine_sim_df,
                         items=data_cbf[['book_title', 'summary', 'book_author', 'rating', 'category']],
                         ):

    # Mengambil data dengan menggunakan argpartition
    # Dataframe diubah menjadi numpy
    # Range(start, stop, step)
    index = similarity_data.loc[:,book_title].to_numpy().argpartition(range(-1, -k, -1))

    # Mengambil data dengan similarity terbesar dari index yang ada
    closest = similarity_data.columns[index[-1:-(k+2):-1]]

    # Drop book_title agar nama resto yang dicari tidak muncul dalam daftar rekomendasi
    closest = closest.drop(book_title, errors='ignore')

    # Mengurutkan items berdasarkan rating tertinggi
    recommendations = pd.DataFrame(closest).merge(items.sort_values(by='rating', ascending=False)).head(k)

    return recommendations

# Selanjutnya, mari kita terapkan kode di atas untuk menemukan rekomendasi restoran yang mirip dengan KFC.
data_cbf[data_cbf.book_title.eq('Death Of Long Steam Lady')]

# Mendapatkan rekomendasi buku yang mirip dengan 'Death Of Long Steam Lady'
book_recommendations('Death Of Long Steam Lady', 7)

"""### Modelling: Collaborative Filtering"""

import tensorflow as tf
from tensorflow import keras
from keras import layers

class RecommenderNet(tf.keras.Model):

  # Insialisasi fungsi
  def __init__(self, num_users, num_isbn_n, embedding_size, **kwargs):
    super(RecommenderNet, self).__init__(**kwargs)
    self.num_users = num_users
    self.num_isbn_n = num_isbn_n
    self.embedding_size = embedding_size
    # layer embedding user
    self.user_embedding = layers.Embedding(
        num_users,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    # layer embedding user bias
    self.user_bias = layers.Embedding(num_users, 1)
    self.isbn_n_embedding = layers.Embedding(
        # layer embeddings isbn
        num_isbn_n,
        embedding_size,
        embeddings_initializer = 'he_normal',
        embeddings_regularizer = keras.regularizers.l2(1e-6)
    )
    # layer embedding isbn bias
    self.isbn_n_bias = layers.Embedding(num_isbn_n, 1)

  def call(self, inputs):
    user_vector = self.user_embedding(inputs[:,0]) # memanggil layer embedding 1
    user_bias = self.user_bias(inputs[:, 0]) # memanggil layer embedding 2
    isbn_n_vector = self.isbn_n_embedding(inputs[:, 1]) # memanggil layer embedding 3
    isbn_n_bias = self.isbn_n_bias(inputs[:, 1]) # memanggil layer embedding 4

    dot_user_isbn_n = tf.tensordot(user_vector, isbn_n_vector, 2)

    x = dot_user_isbn_n + user_bias + isbn_n_bias

    return tf.nn.sigmoid(x) # activation sigmoid

## Proses compile terhadap model.

model = RecommenderNet(num_users, num_isbn, 50) # inisialisasi model

# Model compile
model.compile(
    loss=tf.keras.losses.BinaryCrossentropy(),
    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),
    metrics=[
        tf.keras.metrics.MeanSquaredError(),  # Metrik MSE
        tf.keras.metrics.Precision(),         # Metrik Presisi
        tf.keras.metrics.Recall()             # Metrik Recall
    ]
)

# Memulai training

history = model.fit(
    x = x_train,
    y = y_train,
    epochs = 20,
    validation_data = (x_val, y_val),
)

print("mse: ", history.history['mean_squared_error'])
print("val mse: ", history.history['mean_squared_error'])
print("precision: ", history.history['precision'])
print("val precision: ", history.history['val_precision'])
print("recall: ", history.history['recall'])
print("val recall: ", history.history['val_recall'])
import matplotlib.pyplot as plt

# Visualisasi Mean Squared Error
plt.figure(figsize=(8, 4))
plt.plot(history.history['mean_squared_error'])
plt.plot(history.history['val_mean_squared_error'])
plt.title('Mean Squared Error')
plt.ylabel('MSE')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')

# Visualisasi Precision
plt.figure(figsize=(8, 4))
plt.plot(history.history['precision'])
plt.plot(history.history['val_precision'])
plt.title('Precision')
plt.ylabel('Precision')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')

# Visualisasi Recall
plt.figure(figsize=(8, 4))
plt.plot(history.history['recall'])
plt.plot(history.history['val_recall'])
plt.title('Recall')
plt.ylabel('Recall')
plt.xlabel('Epoch')
plt.legend(['Train', 'Validation'], loc='upper right')

"""#### Rekomendasi dengan predict() dari library keras"""

# Mengambil sample user
user_id = data_rating.user_id.sample(1).iloc[0]
book_review_by_user = data_rating[data_rating.user_id == user_id]

book_not_review = data_books_new[~data_books_new['isbn'].isin(book_review_by_user.isbn.values)]['isbn']
book_not_review = list(
    set(book_not_review)
    .intersection(set(isbn_to_isbn_encoded.keys()))
)

book_not_review = [[isbn_to_isbn_encoded.get(x)] for x in book_not_review]
user_encoder = user_to_user_encoded.get(user_id)
user_resto_array = np.hstack(
    ([[user_encoder]] * len(book_not_review), book_not_review)
)

ratings = model.predict(user_resto_array).flatten()

top_ratings_indices = ratings.argsort()[-10:][::-1]
recommended_book_ids = [
    isbn_encoded_to_isbn.get(book_not_review[x][0]) for x in top_ratings_indices
]

print('Showing recommendations for users: {}'.format(user_id))
print('===' * 12)
print('Book with high ratings from user')
print('===' * 12)

top_books_user = (
    book_review_by_user.sort_values(
        by = 'rating',
        ascending=False
    )
    .head(5)
    .isbn.values
)

book_df_rows = data_books_new[data_books_new['isbn'].isin(top_books_user)]
for row in book_df_rows.itertuples():
    print(row.book_title, ':', row.category)

print('---' * 12)
print('Top 10 Book Recommendation')
print('---' * 12)

recommended_resto = data_books_new[data_books_new['isbn'].isin(recommended_book_ids)]
for row in recommended_resto.itertuples():
    print(row.book_title, ':', row.category)